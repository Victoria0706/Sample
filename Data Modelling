## Step 1 - Import Necessary Library

import os
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import sklearn

## Step 2 - Finding Missing Value 

df.isnull().sum()

## Step 3 - Map the Data using Heatmap

import warnings
warnings.filterwarnings("ignore")

sns.heatmap(df.corr(), cmap="YlGnBu")
plt.show()

## Step 4 - Stratified Shuffle Split 

from sklearn.model_selection import StratifiedShuffleSplit 
split = StratifiedShuffleSplit(n_splits=1,test_size=0.2)
for train_indices, test_indices in split.split(df, df[["Survived", "Pclass", "Sex"]]):
    strat_train_set = df.loc[train_indices]
    strat_test_set = df.loc[test_indices]

strat_test_set

## Step 5 - Plot the Data 

plt.subplot(1,2,1)
strat_train_set['Survived'].hist()
strat_train_set['Pclass'].hist()

plt.subplot(1,2,2)
strat_test_set['Survived'].hist()
strat_test_set['Pclass'].hist()

plt.show()

## Step 6 - AgeImputer (To Fill in Missing Age)

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.impute import SimpleImputer

class AgeImputer(BaseEstimator, TransformerMixin):

    def fit(sel, x, y=None):
        return self 
    def transform(self, x):
        imputer = SimpleImputer(strategy="mean")
        x['Age'] = imputer.fit_transform(x[['Age']])
        return x

## Step 7 - FeatureEncoder (Embarked, Sex)

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import OneHotEncoder
class FeatureEncoder(BaseEstimator, TransformerMixin):
    def fit(self, x, y=None):
        return self 
    def transform(self, x):
        encoder = OneHotEncoder()
        matrix = encoder.fit_transform(x[['Embarked']]).toarray()
        column_names = ["C", "S", "Q", "N"]
        for i in range(len(matrix.T)):
            x[column_names[i]] = matrix.T[i]
        matrix = encoder.fit_transform(x[['Sex']]).toarray()
        column_names =["Female", "Male"]
        for i in range(len(matrix.T)):
            x[column_names[i]] = matrix.T[i]
            return x 

## Step 8 - FeatureDropper (To Drop the Category)

class FeatureDropper(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return X.drop(["Embarked", "Name", "Ticket", "Cabin", "Sex", "N"], axis=1, erros="ignore")

## Step 9 - Pipeline

from sklearn.pipeline import Pipeline 

pipeline = Pipeline([("ageimputer", AgeImputer()),
                    ("featureencoder", FeatureEncoder()), 
                    ("featuredropper", FeatureDropper())])

strat_train_set = pipeline.fit_transform(strat_train_set)
strat_train_set 

## Step 10 -Standard Scaler 

from sklearn.preprocessing import StandardScaler
X = strat_train_set.drop(['Survived'], axis=1)
y= strat_train_set['Survived']
scaler = StandardScaler()
X_data = scaler.fit_transform(X)
y_data = y.to_numpy()

## Step 10 - Random Forest Classifier 

from sklearn.ensemble import RandomForestClassifier 
from sklearn.model_selection import GridSearchCV

clf = RandomForestClassifier()
param_grid = [
    {"n_estimators":[10, 100, 200, 500], "max_depth":[None, 5, 10], "min_samples_split": [2, 3,4]}
]
grid_search = GridSearchCV(clf, param_grid, cv = 3, scoring="accuracy", return_train_score=True)
grid_search.fit(X_data, y_data)

final_clf =grid_search.best_estimator_
final_clf

## Step 11 - Transform the Test Data 

strat_test_set = pipeline.fit_transform(strat_test_set)
X_test = strat_test_set.drop(['Survived'], axis=1)
y_test = strat_test_set['Survived']

scaler = StandardScaler()
X_data_test = scaler.fit_transform(X_test)
y_data_test = y_test.to_numpy()

final_clf.score(X_data_test, y_data_test)

## Step 12 - Transform the Final Data 

final_data = pipeline.fit_transform(df)
final_data

X_final = final_data.drop(['Survived'], axis =1)
y_final = final_data['Survived']

scaler = StandardScaler()
X_data_final =scaler.fit_transform(X_final)
y_data_final = y_final.to_numpy()

## Step 13 - Production Data with Random Forest Classifier 

prod_clf = RandomForestClassifier()
param_grid = [
    {"n_estimators":[10, 100, 200, 500], "max_depth":[None, 5, 10], "min_samples_split": [2, 3,4]}
]
grid_search = GridSearchCV(prod_clf, param_grid, cv = 3, scoring="accuracy", return_train_score=True)
grid_search.fit(X_data_final, y_data_final)

prod_final_clf = grid_search.best_estimator_
prod_final_clf

## Step 14 - Final Predictions 

titanic_test_data = pd.read_csv("data/test.csv")
final_test_data = pipeline.fit_transform(titanic_test_data)
final_test_data

X_final_test = final_test_data
X_final_test = X_final_test.fillna(method="ffill")
scaler = StandardScaler()
X_data_final_test = scaler.fit_transform(X_final_test)

predictions = prod_final_clf.predict(X_data_final_test)

## Step 15 - Save the data into CSV File 

final_df = pd.DataFrame(titanic_test_data["PassengerId"])
final_df['Survived'] = predictions
final_df.to_csv("data/predictions.csv", index=False)

